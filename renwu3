# ============ 1. å¯¼å…¥åº“ ============
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms, models
from torchvision.models import ResNet18_Weights
from PIL import Image
from pathlib import Path
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
import warnings
warnings.filterwarnings('ignore')

# ============ 2. æ•°æ®åˆ†æ ============
def analyze_dataset(data_path):
    """
    åˆ†ææ•°æ®é›†æ„æˆ
    """
    print("=" * 50)
    print("æ•°æ®é›†åˆ†ææŠ¥å‘Š")
    print("=" * 50)
    
    # åœ¨ç±»åˆ«åˆ†å¸ƒéƒ¨åˆ†æ·»åŠ æ•°å­—æ ‡ç­¾
    emotion_classes = ["Angry", "Fear", "Happy", "Sad", "Surprise", "Neutral"]
    emotion_nums = ["0", "1", "2", "3", "4", "5"]  # å¯¹åº”çš„æ•°å­—æ ‡ç­¾
    
    train_path = os.path.join(data_path, "train")
    test_path = os.path.join(data_path, "test")
    
    # æ£€æŸ¥ç›®å½•ç»“æ„
    print(f"è®­ç»ƒé›†è·¯å¾„: {train_path}")
    print(f"æµ‹è¯•é›†è·¯å¾„: {test_path}")
    
    # åˆ†æè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ
    if os.path.exists(train_path):
        class_counts = {}
        
        print("\nè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:")
        for i, emotion in enumerate(emotion_classes):
            emotion_dir = os.path.join(train_path, emotion)
            if os.path.exists(emotion_dir):
                count = len([f for f in os.listdir(emotion_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
                class_counts[emotion] = count
                print(f"  {emotion} (ç±»åˆ« {emotion_nums[i]}): {count} å¼ å›¾ç‰‡")
        
        # ç»˜åˆ¶ç±»åˆ«åˆ†å¸ƒå›¾
        plt.figure(figsize=(12, 5))
        
        # åˆ›å»ºxè½´æ ‡ç­¾ï¼ˆæ˜¾ç¤ºæ•°å­—ï¼‰
        x_labels = [f"{emotion}\n(ç±»åˆ« {num})" for emotion, num in zip(emotion_classes, emotion_nums)]
        
        bars = plt.bar(x_labels, [class_counts.get(e, 0) for e in emotion_classes])
        plt.title('è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ', fontsize=14)
        plt.xlabel('æƒ…æ„Ÿç±»åˆ«ï¼ˆæ•°å­—æ ‡ç­¾ï¼‰', fontsize=12)
        plt.ylabel('æ ·æœ¬æ•°é‡', fontsize=12)
        plt.xticks(rotation=45)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
        for bar, count in zip(bars, [class_counts.get(e, 0) for e in emotion_classes]):
            if count > 0:
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, 
                        str(count), ha='center', va='bottom', fontsize=10)
        
        plt.tight_layout()
        plt.show()
        
        # åˆ†ææ ·æœ¬æ€»æ•°
        total_samples = sum(class_counts.values())
        print(f"\nè®­ç»ƒé›†æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"ç±»åˆ«æ•°é‡: {len(class_counts)}")
        
        # æ£€æŸ¥ç±»åˆ«ä¸å¹³è¡¡
        if class_counts:
            max_count = max(class_counts.values())
            min_count = min(class_counts.values())
            imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
            
            print(f"\nç±»åˆ«ä¸å¹³è¡¡åˆ†æ:")
            print(f"  æœ€å¤šæ ·æœ¬ç±»åˆ«: {max_count}")
            print(f"  æœ€å°‘æ ·æœ¬ç±»åˆ«: {min_count}")
            print(f"  ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}")
            
            if imbalance_ratio > 5:
                print("  âš ï¸  è­¦å‘Š: å­˜åœ¨æ˜æ˜¾çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜!")
            else:
                print("  âœ… ç±»åˆ«åˆ†å¸ƒç›¸å¯¹å‡è¡¡")
    
    # åˆ†ææµ‹è¯•é›†
    if os.path.exists(test_path):
        test_files = [f for f in os.listdir(test_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
        print(f"\næµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_files)}")
    
    print("=" * 50)

# ============ 3. æ•°æ®å¢å¼ºä¸é¢„å¤„ç† ============
class EmotionDataAugmentation:
    """æ•°æ®å¢å¼ºç­–ç•¥"""
    
    @staticmethod
    def get_train_transforms():
        """è®­ç»ƒé›†æ•°æ®å¢å¼ºå˜æ¢"""
        return transforms.Compose([
            transforms.Grayscale(num_output_channels=1),
            transforms.Resize((48, 48)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=10),
            transforms.ColorJitter(brightness=0.2, contrast=0.2),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5], std=[0.5])
        ])
    
    @staticmethod
    def get_val_transforms():
        """éªŒè¯é›†/æµ‹è¯•é›†å˜æ¢"""
        return transforms.Compose([
            transforms.Grayscale(num_output_channels=1),
            transforms.Resize((48, 48)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5], std=[0.5])
        ])

# ============ 4. æ•°æ®é›†ç±»ï¼ˆæ”¯æŒäº¤å‰éªŒè¯ï¼‰ ============
class FERDataset(Dataset):
    def __init__(self, image_dir, mode='train', transform=None, fold_idx=None, total_folds=5):
        self.image_dir = Path(image_dir)
        self.mode = mode
        self.transform = transform
        self.samples = []
        self.class_to_idx = {
            "Angry": 0, "Fear": 1, "Happy": 2, 
            "Sad": 3, "Surprise": 4, "Neutral": 5
        }
        self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}
        self.fold_idx = fold_idx
        self.total_folds = total_folds
        
        self._load_data()
        
        # å¦‚æœæ˜¯è®­ç»ƒæ¨¡å¼ä¸”æŒ‡å®šäº†foldï¼Œåˆ™è¿›è¡Œäº¤å‰éªŒè¯åˆ’åˆ†
        if self.mode == 'train' and fold_idx is not None:
            self._kfold_split()
    
    def _load_data(self):
        """åŠ è½½æ•°æ®"""
        if self.mode == "train":
            # æœ‰æ ‡ç­¾çš„è®­ç»ƒæ•°æ®
            for emotion_name, label in self.class_to_idx.items():
                emotion_dir = self.image_dir / emotion_name
                if emotion_dir.exists():
                    for img_file in emotion_dir.glob("*"):
                        if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:
                            self.samples.append((str(img_file), label))
        else:
            # æ— æ ‡ç­¾çš„æµ‹è¯•æ•°æ®
            for img_file in self.image_dir.glob("*"):
                if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:
                    self.samples.append((str(img_file), -1))  # -1è¡¨ç¤ºæ— æ ‡ç­¾
    
    def _kfold_split(self):
        """KæŠ˜äº¤å‰éªŒè¯åˆ’åˆ†"""
        # æŒ‰ç±»åˆ«åˆ†ç»„ä»¥ä¿æŒåˆ†å¸ƒ
        from collections import defaultdict
        class_samples = defaultdict(list)
        
        for img_path, label in self.samples:
            class_samples[label].append((img_path, label))
        
        train_samples = []
        val_samples = []
        
        for label, samples in class_samples.items():
            n_samples = len(samples)
            fold_size = n_samples // self.total_folds
            
            # è®¡ç®—å½“å‰foldçš„ç´¢å¼•èŒƒå›´
            start_idx = self.fold_idx * fold_size
            end_idx = (self.fold_idx + 1) * fold_size if self.fold_idx < self.total_folds - 1 else n_samples
            
            # éªŒè¯é›†
            val_samples.extend(samples[start_idx:end_idx])
            # è®­ç»ƒé›†
            train_samples.extend(samples[:start_idx] + samples[end_idx:])
        
        # æ ¹æ®æ¨¡å¼è¿”å›ç›¸åº”æ ·æœ¬
        if 'train' in self.mode:
            self.samples = train_samples
        elif 'val' in self.mode:
            self.samples = val_samples
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')
        
        # åº”ç”¨å˜æ¢
        if self.transform:
            image = self.transform(image)
        else:
            # é»˜è®¤å˜æ¢
            image = transforms.Compose([
                transforms.Grayscale(num_output_channels=1),
                transforms.Resize((48, 48)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.5], std=[0.5])
            ])(image)
        
        if self.mode == 'test':
            return image, img_path
        else:
            return image, label
    
    def get_class_distribution(self):
        """è·å–ç±»åˆ«åˆ†å¸ƒ"""
        if self.mode == 'test':
            return {}
        
        distribution = {class_name: 0 for class_name in self.class_to_idx.keys()}
        for _, label in self.samples:
            if label != -1:
                class_name = self.idx_to_class[label]
                distribution[class_name] += 1
        
        return distribution

# ============ 5. æ¨¡å‹æ¶æ„ï¼ˆä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹+æ³¨æ„åŠ›ï¼‰ ============
class AttentionBlock(nn.Module):
    """æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—"""
    def __init__(self, in_channels):
        super().__init__()
        self.query = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.key = nn.Conv2d(in_channels, in_channels // 8, 1)
        self.value = nn.Conv2d(in_channels, in_channels, 1)
        self.gamma = nn.Parameter(torch.zeros(1))
        
    def forward(self, x):
        batch_size, C, H, W = x.size()
        
        # è®¡ç®—query, key, value
        query = self.query(x).view(batch_size, -1, H * W).permute(0, 2, 1)
        key = self.key(x).view(batch_size, -1, H * W)
        value = self.value(x).view(batch_size, -1, H * W)
        
        # è®¡ç®—æ³¨æ„åŠ›æƒé‡
        attention = torch.bmm(query, key)
        attention = torch.softmax(attention, dim=-1)
        
        # åº”ç”¨æ³¨æ„åŠ›
        out = torch.bmm(value, attention.permute(0, 2, 1))
        out = out.view(batch_size, C, H, W)
        
        # æ®‹å·®è¿æ¥
        out = self.gamma * out + x
        
        return out

class EmotionCNNWithAttention(nn.Module):
    """å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„CNNæ¨¡å‹"""
    def __init__(self, num_classes=6, pretrained=True, use_attention=True):
        super().__init__()
        self.use_attention = use_attention
        
        # åŠ è½½é¢„è®­ç»ƒçš„ResNet18
        if pretrained:
            weights = ResNet18_Weights.IMAGENET1K_V1
        else:
            weights = None
        
        self.backbone = models.resnet18(weights=weights)
        
        # ä¿®æ”¹ç¬¬ä¸€å±‚ä»¥æ¥å—1é€šé“è¾“å…¥
        original_conv1 = self.backbone.conv1
        self.backbone.conv1 = nn.Conv2d(
            1,  # è¾“å…¥é€šé“æ”¹ä¸º1ï¼ˆç°åº¦å›¾ï¼‰
            original_conv1.out_channels,
            kernel_size=original_conv1.kernel_size,
            stride=original_conv1.stride,
            padding=original_conv1.padding,
            bias=original_conv1.bias
        )
        
        # åˆå§‹åŒ–æ–°çš„ç¬¬ä¸€å±‚æƒé‡
        if pretrained:
            with torch.no_grad():
                self.backbone.conv1.weight = nn.Parameter(
                    original_conv1.weight.mean(dim=1, keepdim=True)
                )
        
        # åœ¨layer2åæ·»åŠ æ³¨æ„åŠ›æ¨¡å—
        if use_attention:
            self.attention = AttentionBlock(128)
        
        # è·å–ç‰¹å¾ç»´åº¦
        num_features = self.backbone.fc.in_features
        
        # æ›¿æ¢å…¨è¿æ¥å±‚
        self.backbone.fc = nn.Sequential(
            nn.Dropout(0.3),
            nn.Linear(num_features, 256),
            nn.ReLU(inplace=True),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        # ResNetçš„å‰å‘ä¼ æ’­ï¼ˆä¿®æ”¹ä»¥è·å–ä¸­é—´ç‰¹å¾ï¼‰
        x = self.backbone.conv1(x)
        x = self.backbone.bn1(x)
        x = self.backbone.relu(x)
        x = self.backbone.maxpool(x)
        
        x = self.backbone.layer1(x)
        x = self.backbone.layer2(x)
        
        # åº”ç”¨æ³¨æ„åŠ›
        if self.use_attention:
            x = self.attention(x)
        
        x = self.backbone.layer3(x)
        x = self.backbone.layer4(x)
        
        x = self.backbone.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.backbone.fc(x)
        
        return x

# ============ 6. è®­ç»ƒå’ŒéªŒè¯å‡½æ•° ============
def train_epoch(model, dataloader, criterion, optimizer, device, scheduler=None):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch_idx, (images, labels) in enumerate(dataloader):
        images, labels = images.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        if scheduler:
            scheduler.step()
        
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
    
    train_loss = total_loss / len(dataloader)
    train_acc = 100. * correct / total
    
    return train_loss, train_acc

def validate(model, dataloader, criterion, device):
    """éªŒè¯æ¨¡å‹"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    val_loss = total_loss / len(dataloader)
    val_acc = 100. * correct / total
    
    return val_loss, val_acc, all_predictions, all_labels

# ============ 7. äº¤å‰éªŒè¯è®­ç»ƒ ============
def cross_validation_train(data_path, num_folds=5, epochs=20, batch_size=64):
    """KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒ"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å‡†å¤‡æ•°æ®
    train_path = os.path.join(data_path, "train")
    
    # å­˜å‚¨æ¯æŠ˜çš„ç»“æœ
    fold_results = []
    
    for fold in range(num_folds):
        print(f"\n{'='*60}")
        print(f"ç¬¬ {fold+1}/{num_folds} æŠ˜äº¤å‰éªŒè¯")
        print(f"{'='*60}")
        
        # åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨å½“å‰æŠ˜ï¼‰
        train_dataset = FERDataset(
            train_path, 
            mode='train_fold', 
            transform=EmotionDataAugmentation.get_train_transforms(),
            fold_idx=fold,
            total_folds=num_folds
        )
        
        val_dataset = FERDataset(
            train_path,
            mode='val_fold',
            transform=EmotionDataAugmentation.get_val_transforms(),
            fold_idx=fold,
            total_folds=num_folds
        )
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True, 
            num_workers=2,
            pin_memory=True
        )
        
        val_loader = DataLoader(
            val_dataset, 
            batch_size=batch_size, 
            shuffle=False, 
            num_workers=2,
            pin_memory=True
        )
        
        # åˆ›å»ºæ¨¡å‹
        model = EmotionCNNWithAttention(
            num_classes=6,
            pretrained=True,
            use_attention=True
        ).to(device)
        
        # ä½¿ç”¨å¤šGPU
        if torch.cuda.device_count() > 1:
            print(f"ä½¿ç”¨ {torch.cuda.device_count()} å—GPU")
            model = nn.DataParallel(model)
        
        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        
        # è®­ç»ƒå†å²è®°å½•
        history = {
            'train_loss': [], 'train_acc': [],
            'val_loss': [], 'val_acc': []
        }
        
        best_val_acc = 0
        best_model_state = None
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            train_loss, train_acc = train_epoch(
                model, train_loader, criterion, optimizer, device, scheduler
            )
            
            val_loss, val_acc, _, _ = validate(
                model, val_loader, criterion, device
            )
            
            # è®°å½•å†å²
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            
            print(f"Epoch {epoch+1}/{epochs}: "
                  f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, "
                  f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_model_state = model.state_dict().copy()
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        torch.save(best_model_state, f'best_model_fold_{fold+1}.pth')
        print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        
        fold_results.append({
            'fold': fold + 1,
            'best_val_acc': best_val_acc,
            'history': history,
            'model_state': best_model_state
        })
    
    # åˆ†æäº¤å‰éªŒè¯ç»“æœ
    print(f"\n{'='*60}")
    print("äº¤å‰éªŒè¯ç»“æœæ±‡æ€»")
    print(f"{'='*60}")
    
    val_accs = [result['best_val_acc'] for result in fold_results]
    mean_val_acc = np.mean(val_accs)
    std_val_acc = np.std(val_accs)
    
    print(f"å„æŠ˜éªŒè¯å‡†ç¡®ç‡: {[f'{acc:.2f}%' for acc in val_accs]}")
    print(f"å¹³å‡éªŒè¯å‡†ç¡®ç‡: {mean_val_acc:.2f}%")
    print(f"æ ‡å‡†å·®: {std_val_acc:.2f}%")
    
    return fold_results, mean_val_acc

# ============ 8. æœ€ç»ˆæ¨¡å‹è®­ç»ƒï¼ˆä½¿ç”¨æ‰€æœ‰æ•°æ®ï¼‰ ============
def train_final_model(data_path, epochs=30, batch_size=128):
    """ä½¿ç”¨æ‰€æœ‰è®­ç»ƒæ•°æ®è®­ç»ƒæœ€ç»ˆæ¨¡å‹"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"è®­ç»ƒæœ€ç»ˆæ¨¡å‹ï¼Œä½¿ç”¨è®¾å¤‡: {device}")
    
    # å‡†å¤‡æ•°æ®
    train_path = os.path.join(data_path, "train")
    test_path = os.path.join(data_path, "test")
    
    # åˆ›å»ºå®Œæ•´è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆ80-20åˆ’åˆ†ï¼‰
    full_dataset = FERDataset(
        train_path,
        mode='train',
        transform=EmotionDataAugmentation.get_train_transforms()
    )
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    
    train_dataset, val_dataset = random_split(
        full_dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    print(f"å®Œæ•´è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"æœ€ç»ˆéªŒè¯é›†å¤§å°: {len(val_dataset)}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        pin_memory=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    # åˆ›å»ºæµ‹è¯•é›†
    test_dataset = FERDataset(
        test_path,
        mode='test',
        transform=EmotionDataAugmentation.get_val_transforms()
    )
    
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = EmotionCNNWithAttention(
        num_classes=6,
        pretrained=True,
        use_attention=True
    ).to(device)
    
    # ä½¿ç”¨å¤šGPU
    if torch.cuda.device_count() > 1:
        print(f"ä½¿ç”¨ {torch.cuda.device_count()} å—GPU")
        model = nn.DataParallel(model)
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=5, verbose=True
    )
    
    # è®­ç»ƒå†å²è®°å½•
    history = {
        'train_loss': [], 'train_acc': [],
        'val_loss': [], 'val_acc': []
    }
    
    best_val_acc = 0
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, device
        )
        
        # éªŒè¯
        val_loss, val_acc, _, _ = validate(
            model, val_loader, criterion, device
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_acc)
        
        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        
        print(f"Epoch {epoch+1}/{epochs}: "
              f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, "
              f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_final_model.pth')
            print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
    
    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    plot_training_history(history)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
    model.load_state_dict(torch.load('best_final_model.pth'))
    
    return model, test_loader, device, history

# ============ 9. ç»˜åˆ¶è®­ç»ƒå†å² ============
def plot_training_history(history):
    """ç»˜åˆ¶è®­ç»ƒå†å²æ›²çº¿"""
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    
    epochs = range(1, len(history['train_loss']) + 1)
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)
    axes[0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)
    axes[0].set_xlabel('Epoch', fontsize=12)
    axes[0].set_ylabel('Loss', fontsize=12)
    axes[0].set_title('Training and Validation Loss', fontsize=14)
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿
    axes[1].plot(epochs, history['train_acc'], 'b-', label='Train Accuracy', linewidth=2)
    axes[1].plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)
    axes[1].set_xlabel('Epoch', fontsize=12)
    axes[1].set_ylabel('Accuracy (%)', fontsize=12)
    axes[1].set_title('Training and Validation Accuracy', fontsize=14)
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# ============ 10. ç”Ÿæˆé¢„æµ‹ç»“æœï¼ˆè¾“å‡ºæ•°å­—æ ‡ç­¾ï¼‰ ============
def generate_predictions(model, test_loader, device, output_csv='submission.csv'):
    """ç”Ÿæˆé¢„æµ‹ç»“æœå¹¶ä¿å­˜ä¸ºCSVï¼ˆè¾“å‡ºæ•°å­—æ ‡ç­¾ï¼‰"""
    model.eval()
    predictions = []
    file_names = []
    
    with torch.no_grad():
        for batch_idx, (images, paths) in enumerate(test_loader):
            images = images.to(device)
            outputs = model(images)
            _, predicted = outputs.max(1)
            
            # å°†é¢„æµ‹ç»“æœæ·»åŠ åˆ°åˆ—è¡¨ä¸­ï¼ˆç›´æ¥æ˜¯æ•°å­—0-5ï¼‰
            predictions.extend(predicted.cpu().numpy())
            
            # å¤„ç†æ–‡ä»¶è·¯å¾„
            for path in paths:
                file_name = Path(path).name
                file_names.append(file_name)
            
            # æ‰“å°è¿›åº¦
            if (batch_idx + 1) % 10 == 0:
                print(f"å¤„ç†è¿›åº¦: {batch_idx + 1}/{len(test_loader)} batches")
    
    # åˆ›å»ºç»“æœDataFrame - Emotionåˆ—ç›´æ¥å­˜å‚¨æ•°å­—0-5
    results_df = pd.DataFrame({
        'ID': file_names,
        'Emotion': predictions  # ç›´æ¥è¾“å‡ºæ•°å­—0-5
    })
    
    # ç¡®ä¿Emotionåˆ—æ˜¯æ•´æ•°ç±»å‹
    results_df['Emotion'] = results_df['Emotion'].astype(int)
    
    # æŒ‰IDæ’åºï¼ˆå¦‚æœéœ€è¦ï¼‰
    results_df = results_df.sort_values('ID')
    
    # ä¿å­˜ä¸ºCSV
    results_df.to_csv(output_csv, index=False)
    print(f"\nâœ… é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_csv}")
    print(f"æ€»é¢„æµ‹æ ·æœ¬æ•°: {len(results_df)}")
    
    # æ˜¾ç¤ºé¢„æµ‹åˆ†å¸ƒï¼ˆä½¿ç”¨æ•°å­—æ ‡ç­¾ï¼‰
    print("\nğŸ“Š é¢„æµ‹ç»“æœåˆ†å¸ƒï¼ˆæ•°å­—æ ‡ç­¾ï¼‰:")
    emotion_counts = results_df['Emotion'].value_counts().sort_index()
    
    # æ•°å­—åˆ°æƒ…æ„Ÿçš„æ˜ å°„ï¼ˆä»…ç”¨äºæ˜¾ç¤ºï¼‰
    emotion_mapping = {
        0: "æ„¤æ€’",
        1: "ææƒ§", 
        2: "å¿«ä¹",
        3: "æ‚²ä¼¤",
        4: "æƒŠè®¶",
        5: "ä¸­æ€§"
    }
    
    for emotion_num, count in emotion_counts.items():
        emotion_name = emotion_mapping.get(emotion_num, f"æœªçŸ¥({emotion_num})")
        percentage = count/len(results_df)*100
        print(f"  ç±»åˆ« {emotion_num} ({emotion_name}): {count} å¼ å›¾ç‰‡ ({percentage:.1f}%)")
    
    return results_df

# ============ 11. ä¸»å‡½æ•° ============
def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®è·¯å¾„ - ä¿®æ”¹è¿™ä¸€è¡Œï¼
    ROOT_DIR = "/kaggle/input/neu-image-emotion-classification/fer_data/fer_data"
    
    print("ğŸš€ å¼€å§‹äººè„¸æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡")
    print("=" * 60)
    
    # æ·»åŠ è·¯å¾„éªŒè¯
    import os
    print(f"æ£€æŸ¥è·¯å¾„: {ROOT_DIR}")
    print(f"è·¯å¾„æ˜¯å¦å­˜åœ¨: {os.path.exists(ROOT_DIR)}")
    
    if os.path.exists(ROOT_DIR):
        print("ç›®å½•å†…å®¹:")
        for item in os.listdir(ROOT_DIR):
            print(f"  - {item}")
    else:
        print("âš ï¸ è­¦å‘Šï¼šè·¯å¾„ä¸å­˜åœ¨ï¼è¯·æ£€æŸ¥è·¯å¾„è®¾ç½®ã€‚")
        return
    
    # 1. æ•°æ®åˆ†æ
    analyze_dataset(ROOT_DIR)
    
    # 2. äº¤å‰éªŒè¯è®­ç»ƒï¼ˆå¯é€‰ï¼‰
    print("\nğŸ”§ å¼€å§‹äº¤å‰éªŒè¯è®­ç»ƒ...")
    try:
        fold_results, mean_cv_acc = cross_validation_train(
            ROOT_DIR,
            num_folds=3,  # å¯ä»¥è®¾ç½®ä¸º5ï¼Œä½†ä¸ºäº†é€Ÿåº¦è¿™é‡Œè®¾ä¸º3
            epochs=15,
            batch_size=128
        )
        print(f"äº¤å‰éªŒè¯å¹³å‡å‡†ç¡®ç‡: {mean_cv_acc:.2f}%")
    except Exception as e:
        print(f"äº¤å‰éªŒè¯æ—¶å‡ºé”™: {e}")
        print("ç»§ç»­ä½¿ç”¨å®Œæ•´è®­ç»ƒ...")
    
    # 3. è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print("\nğŸ”¥ è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    model, test_loader, device, history = train_final_model(
        ROOT_DIR,
        epochs=30,
        batch_size=128
    )
    
    # 4. ç”Ÿæˆé¢„æµ‹ç»“æœ
    print("\nğŸ“Š ç”Ÿæˆé¢„æµ‹ç»“æœ...")
    results_df = generate_predictions(
        model,
        test_loader,
        device,
        output_csv='emotion_predictions.csv'
    )
    
    # 5. æ˜¾ç¤ºé¢„æµ‹ç»“æœç¤ºä¾‹
    print("\nğŸ“‹ é¢„æµ‹ç»“æœç¤ºä¾‹ï¼ˆå‰10è¡Œï¼‰:")
    print(results_df.head(10))
    
    print("\nâœ… ä»»åŠ¡å®Œæˆï¼")
    print(f"é¢„æµ‹ç»“æœä¿å­˜åœ¨: emotion_predictions.csv")
    print("\nâš ï¸ æ³¨æ„ï¼šCSVæ–‡ä»¶ä¸­çš„Emotionåˆ—æ˜¯æ•°å­—æ ‡ç­¾ï¼Œå¯¹åº”å…³ç³»å¦‚ä¸‹ï¼š")
    print("  0: æ„¤æ€’, 1: ææƒ§, 2: å¿«ä¹, 3: æ‚²ä¼¤, 4: æƒŠè®¶, 5: ä¸­æ€§")

# ============ 12. æ‰§è¡Œ ============
if __name__ == '__main__':
    main()
